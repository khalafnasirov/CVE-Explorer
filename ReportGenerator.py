import requests
from bs4 import BeautifulSoup
from event_manager import EventManager
import DataValues as dv

r = None
event_manager = EventManager()

def ResetAllDataValues():
    dv.header = ""
    dv.description = ""
    dv.nist = ""
    dv.base_score = ""
    dv.vector = ""
    dv.hyperlinks = []
    dv.hyperlinks_count = 0
    dv.exploited_vulner_name = ""
    dv.exploited_vulner_added_date = ""
    dv.exploited_vulner_required_action = ""
    dv.cwe_names = []
    dv.publish_time = ""
    dv.source = ""
    dv.events = {}
    dv.running_url = ""
    dv.exploit_database_url = ""
    dv.no_information = False
    dv.input_cve = ""
    dv.exploitdb_status = False
    dv.download_path = ""
    dv.status = False
    dv.exploit_db_links = []
    dv.exploitdb_status = False
    dv.is_exploitdb_searched = False

def CheckDataSource(datasource):
    if datasource == "":
        datasource = "No Information"
        return datasource
    else:
        return datasource

def GenerateReport(cve_input):
    cve_input = str(cve_input).upper()
    r = requests.get(f"https://nvd.nist.gov/vuln/detail/CVE-{cve_input}")
    dv.running_url = r.url

    # Handle process
    if not r.ok:
        dv.status = False
        event_manager.trigger_event("handle_link_event", dv.status)
    else:
        GetResults(r, cve_input)
        dv.status = True
        event_manager.trigger_event("handle_link_event", dv.status)  
    

def GetResults(request, cve):
    # Parse the HTML content of the page
    soup = BeautifulSoup(request.text, 'html.parser')
    # CVE Information
    s_header = soup.find(attrs={"data-testid": "page-header-vuln-id"})
    s_description = soup.find(attrs={"data-testid": "vuln-description"})
    s_nist = soup.find(attrs={"data-testid": "vuln-cvss3-source-nvd"})
    s_base_score = soup.find(attrs={"id":"Cvss3NistCalculatorAnchor"})
    s_vector = soup.find(attrs={"data-testid":"vuln-cvss3-nist-vector"})
    link_table = soup.find('table', {'data-testid': 'vuln-hyperlinks-table'})
    if link_table is not None:
        a_links = link_table.find_all('a') # type: ignore
        if a_links is not None:
            for link in a_links:
                href = link.get('href')
                dv.hyperlinks.append(href)
    weakness_table = soup.find(attrs={'id': 'vulnTechnicalDetailsDiv'})
    s_cwe_names = []
    if weakness_table:
        table = weakness_table.find('table')
        cwe_rows = len(table) - 1 # type: ignore
        for i in range(cwe_rows):
            s_cwe_names.append(soup.find(attrs={"data-testid":f"vuln-CWEs-link-{i}"}))
    s_publish_time = soup.find(attrs={"data-testid":"vuln-published-on"})
    s_source = soup.find(attrs={"data-testid":"vuln-current-description-source"})

    if s_header is not None: dv.header = s_header.text
    if s_base_score is not None: dv.base_score = s_base_score.text
    if s_description is not None: dv.description = s_description.text
    if s_nist is not None: dv.nist = s_nist.text
    for cwe_name in s_cwe_names:
        if cwe_name is not None:
            dv.cwe_names.append(cwe_name.text)
    if s_publish_time is not None: dv.publish_time = s_publish_time.text
    if s_source is not None: dv.source = s_source.text
    if s_vector is not None: dv.vector = s_vector.text
    dv.exploit_database_url = f"https://www.exploit-db.com/search?cve={cve}"